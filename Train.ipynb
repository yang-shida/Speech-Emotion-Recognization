{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa, librosa.display\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation (noise)\n",
    "def manipulate(data, noise_factor):\n",
    "    noise = np.random.randn(len(data[0]))\n",
    "    augmented_data = data + noise_factor * noise\n",
    "    # Cast back to same data type\n",
    "    augmented_data = augmented_data.astype(type(data[0]))\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_train(X_train_raw, y_train, model_save_path):\n",
    "    \n",
    "    print(\"Input audio shape: \", X_train_raw.shape)\n",
    "    \n",
    "    # Data augmentation (add noise)\n",
    "    print(\"Performing data augmentation (adding noise)...\")\n",
    "    aug_train = manipulate(X_train_raw, 0)\n",
    "    X_train_raw = np.concatenate((X_train_raw, aug_train))\n",
    "    y_train = np.concatenate((y_train, y_train))\n",
    "    \n",
    "    # Shuffle data (with augmentation)\n",
    "    shuffler = np.random.permutation(len(X_train_raw))\n",
    "    X_train_raw = X_train_raw[shuffler]\n",
    "    y_train = y_train[shuffler]\n",
    "    \n",
    "    # Min-max normalization\n",
    "    X_train_raw_scaled = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(X_train_raw.T)\n",
    "    X_train_raw_scaled = X_train_raw_scaled.transform(X_train_raw.T).T\n",
    "    X_train_raw = X_train_raw_scaled\n",
    "    print(\"Training audio shape: \", X_train_raw.shape)\n",
    "    \n",
    "    # Extract MFCC feature\n",
    "    print(\"Extracting MFCC feature...\")\n",
    "    sr = 44100\n",
    "    n_mfcc = 13\n",
    "    n_fft_mfcc = 2048\n",
    "    hop_length_mfcc = 512\n",
    "\n",
    "    MFCC_feature_matrix = []\n",
    "    for i in range(X_train_raw.shape[0]):\n",
    "        MFCC_feature_matrix += [librosa.feature.mfcc(X_train_raw[i], sr = sr, n_mfcc = n_mfcc, n_fft = n_fft_mfcc, hop_length = hop_length_mfcc)]\n",
    "    MFCC_feature_matrix = np.array(MFCC_feature_matrix)\n",
    "    print(\"MFCC feature shape: \", MFCC_feature_matrix.shape)\n",
    "\n",
    "    # Add a depth of 1 so the data can be used in CNN\n",
    "    X_train_total = MFCC_feature_matrix[..., np.newaxis]\n",
    "\n",
    "    # One-hot encode output\n",
    "    y_train_total = to_categorical(y_train)\n",
    "    \n",
    "    # CNN parameters\n",
    "    EPOCHS = 120\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 0.0005\n",
    "    NUM_OF_ENSEMBLE_MODEL = 20\n",
    "\n",
    "    for i in range(NUM_OF_ENSEMBLE_MODEL):\n",
    "        print(\"Training model number \", i)\n",
    "        \n",
    "        sample_ind = np.random.choice(np.array(range(X_train_total.shape[0])), int(X_train_total.shape[0]*0.8), replace=False)\n",
    "        X_train = X_train_total[sample_ind]\n",
    "        y_train = y_train_total[sample_ind]\n",
    "        \n",
    "        # Build CNN model\n",
    "        model = keras.Sequential()\n",
    "        # conv layer 1\n",
    "        model.add(keras.layers.Conv2D(filters=32, kernel_size=(10, 10), activation=\"relu\", padding=\"same\",\n",
    "                                      input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]),\n",
    "                                      kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.MaxPool2D((3, 3), strides=(2, 2), padding=\"same\"))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(filters=32, kernel_size=(7, 7), activation=\"relu\", padding=\"same\", \n",
    "                                      kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "        # conv layer 2\n",
    "        model.add(keras.layers.Conv2D(filters=64, kernel_size=(7, 7), activation=\"relu\", padding=\"same\", \n",
    "                                      kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.MaxPool2D((3, 3), strides=(2, 2), padding=\"same\"))\n",
    "        model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "        # conv layer 3\n",
    "        model.add(keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", \n",
    "                                      kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.MaxPool2D((3, 3), strides=(2, 2), padding=\"same\"))\n",
    "        model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "        # flatten output\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(512, activation=\"relu\"))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(0.5))\n",
    "        model.add(keras.layers.Dense(256, activation=\"relu\"))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(0.5))\n",
    "        model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "        # softmax classifier\n",
    "        model.add(keras.layers.Dense(9, activation=\"softmax\"))\n",
    "\n",
    "        # compile model\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "        # need to use sparse_categorical_crossentropy since our output are integers not one-hot encoded\n",
    "        model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "        # train model\n",
    "        history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, verbose=0)\n",
    "\n",
    "        # save model\n",
    "        model.save(model_save_path+'/model'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_train(X_train, y_train, model_save_path):\n",
    "    # KNN parameters\n",
    "    N_NEIGHBORS = 3\n",
    "    METRIC = 'manhattan'\n",
    "    WEIGHTS = 'distance'\n",
    "    \n",
    "    # MFCC\n",
    "    sr = 44100\n",
    "    n_mfcc = 13\n",
    "    n_fft_mfcc = 2048\n",
    "    hop_length_mfcc = 512\n",
    "\n",
    "    print(\"Extracting MFCC feature...\")\n",
    "    MFCC_feature_matrix = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        MFCC_feature_matrix += [librosa.feature.mfcc(X_train[i], sr = sr, n_mfcc = n_mfcc, n_fft = n_fft_mfcc, hop_length = hop_length_mfcc)]\n",
    "    MFCC_feature_matrix = np.array(MFCC_feature_matrix)\n",
    "    MFCC_feature_matrix = np.mean(MFCC_feature_matrix, axis=2)\n",
    "    \n",
    "    # STFT\n",
    "    n_fft_stft = 4096\n",
    "    hop_length_stft = 2048\n",
    "\n",
    "    print(\"Extracting STFT feature...\")\n",
    "    STFT_feature_matrix = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        STFT_feature_matrix += [np.abs(librosa.core.stft(X_train[i], n_fft = n_fft_stft, hop_length = hop_length_stft))]\n",
    "    STFT_feature_matrix = np.array(STFT_feature_matrix)\n",
    "    STFT_feature_matrix = np.mean(STFT_feature_matrix, axis=2)\n",
    "    \n",
    "    FeatureMatrix = np.concatenate((MFCC_feature_matrix, STFT_feature_matrix), axis=1)\n",
    "    \n",
    "    knn_class_1 = KNeighborsClassifier(n_neighbors = N_NEIGHBORS, \n",
    "                                 metric = METRIC,\n",
    "                                 weights = WEIGHTS)\n",
    "    \n",
    "    print(\"Training KNN model...\")\n",
    "    model = knn_class_1.fit(FeatureMatrix, y_train)\n",
    "    \n",
    "    # save model\n",
    "    knnPickle = open(model_save_path, 'wb') \n",
    "    pickle.dump(model, knnPickle)\n",
    "    \n",
    "    print(\"K-NN training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train):\n",
    "    \n",
    "    # create folder to put ensemble learners\n",
    "    if not os.path.exists('Ensemble_Learners'):\n",
    "        os.makedirs('Ensemble_Learners')\n",
    "        \n",
    "    # train CNN and save model\n",
    "    print(\"Training ensemble learners...\")\n",
    "    cnn_train(X_train, y_train, 'Ensemble_Learners')\n",
    "    # train KNN and save model\n",
    "    print(\"Training knn...\")\n",
    "    knn_train(X_train, y_train, 'model_knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "# load data\n",
    "# 2400 x 100000\n",
    "data_training = np.load('data_training.npy')\n",
    "# 1 - 8 (2400 x 1)\n",
    "labels_training = np.load('labels_training.npy')\n",
    "\n",
    "# label is from 1-8, so I added a place holder at position 0\n",
    "labels_names = ['place holder', 'neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING/TEST SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_raw, X_test, y_train, y_test = train_test_split(data_training, labels_training, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train/test split for future testing\n",
    "np.savetxt('X_train_raw.csv', X_train_raw, delimiter=\",\")\n",
    "np.savetxt('y_train.csv', y_train, delimiter=\",\")\n",
    "np.savetxt('X_test.csv', X_test, delimiter=\",\")\n",
    "np.savetxt('y_test.csv', y_test, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train/test split\n",
    "X_train_raw = np.genfromtxt('X_train_raw.csv', delimiter=',')\n",
    "y_train = np.genfromtxt('y_train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt('X_test.csv', delimiter=',')\n",
    "y_test = np.genfromtxt('y_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble learners...\n",
      "Input audio shape:  (1920, 100000)\n",
      "Performing data augmentation (adding noise)...\n",
      "Training audio shape:  (3840, 100000)\n",
      "Extracting MFCC feature...\n",
      "MFCC feature shape:  (3840, 13, 196)\n",
      "Training model number  0\n",
      "Training model number  1\n",
      "Training model number  2\n",
      "Training model number  3\n",
      "Training model number  4\n",
      "Training model number  5\n",
      "Training model number  6\n",
      "Training model number  7\n",
      "Training model number  8\n",
      "Training model number  9\n",
      "Training model number  10\n",
      "Training model number  11\n",
      "Training model number  12\n",
      "Training model number  13\n",
      "Training model number  14\n",
      "Training model number  15\n",
      "Training model number  16\n",
      "Training model number  17\n",
      "Training model number  18\n",
      "Training model number  19\n",
      "Training knn...\n",
      "Extracting MFCC feature...\n",
      "Extracting STFT feature...\n",
      "Training KNN model...\n",
      "K-NN training completed\n"
     ]
    }
   ],
   "source": [
    "# train and save model\n",
    "train(X_train_raw, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

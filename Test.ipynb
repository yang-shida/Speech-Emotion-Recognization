{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa, librosa.display\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label is from 1-8, so I added a place holder at position 0\n",
    "labels_names = ['place holder', 'neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test, y_test):\n",
    "    \n",
    "    # run ensemble learners\n",
    "    # min-max normalize\n",
    "    X_test_scaled = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(X_test.T)\n",
    "    X_test_scaled = X_test_scaled.transform(X_test.T).T\n",
    "    \n",
    "    # compute MFCC\n",
    "    sr = 44100\n",
    "    n_mfcc = 13\n",
    "    n_fft_mfcc = 2048\n",
    "    hop_length_mfcc = 512\n",
    "\n",
    "    MFCC_feature_matrix = []\n",
    "    for i in range(X_test_scaled.shape[0]):\n",
    "        MFCC_feature_matrix += [librosa.feature.mfcc(X_test_scaled[i], sr = sr, n_mfcc = n_mfcc, n_fft = n_fft_mfcc, hop_length = hop_length_mfcc)]\n",
    "    MFCC_feature_matrix = np.array(MFCC_feature_matrix)\n",
    "\n",
    "    # Add a depth of 1 so the data can be used in CNN\n",
    "    X_test1 = MFCC_feature_matrix[..., np.newaxis]\n",
    "    \n",
    "    # compute output softmax\n",
    "    pred_total = np.zeros((X_test_scaled.shape[0], 9))\n",
    "\n",
    "    # run each learner and add their softmax output\n",
    "    for i in range(20):\n",
    "        model = keras.models.load_model('Ensemble_Learners/model'+str(i)+'.h5')\n",
    "        pred_class = model.predict(X_test1)\n",
    "\n",
    "        pred_total += pred_class\n",
    "        \n",
    "    # run knn\n",
    "    # MFCC\n",
    "    sr = 44100\n",
    "    n_mfcc = 13\n",
    "    n_fft_mfcc = 2048\n",
    "    hop_length_mfcc = 512\n",
    "\n",
    "    MFCC_feature_matrix = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        MFCC_feature_matrix += [librosa.feature.mfcc(X_test[i], sr = sr, n_mfcc = n_mfcc, n_fft = n_fft_mfcc, hop_length = hop_length_mfcc)]\n",
    "    MFCC_feature_matrix = np.array(MFCC_feature_matrix)\n",
    "    MFCC_feature_matrix = np.mean(MFCC_feature_matrix, axis=2)\n",
    "\n",
    "    # STFT\n",
    "    n_fft_stft = 4096\n",
    "    hop_length_stft = 2048\n",
    "\n",
    "    STFT_feature_matrix = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        STFT_feature_matrix += [np.abs(librosa.core.stft(X_test[i], n_fft = n_fft_stft, hop_length = hop_length_stft))]\n",
    "    STFT_feature_matrix = np.array(STFT_feature_matrix)\n",
    "    STFT_feature_matrix = np.mean(STFT_feature_matrix, axis=2)\n",
    "\n",
    "    FeatureMatrix = np.concatenate((MFCC_feature_matrix, STFT_feature_matrix), axis=1)\n",
    "\n",
    "    # load knn model and make prediction\n",
    "    loaded_model = pickle.load(open('model_knn', 'rb'))\n",
    "    pred_yp_knn_s1 = loaded_model.predict(FeatureMatrix)\n",
    "    \n",
    "    # get the top3 choice of ensemble learners\n",
    "    cnn_1st_ind = pred_total.argsort()[:,-1]\n",
    "    cnn_2st_ind = pred_total.argsort()[:,-2]\n",
    "    cnn_3st_ind = pred_total.argsort()[:,-3]\n",
    "\n",
    "    THRESHOLD = 4\n",
    "\n",
    "    combined_class_pred = []\n",
    "\n",
    "    for i in range(len(pred_yp_knn_s1)):\n",
    "        # if first choice is THRESHOLD times or greater than the second choice, we choose first choice\n",
    "        if pred_total[i][cnn_1st_ind[i]] > 4*pred_total[i][cnn_2st_ind[i]]:\n",
    "            combined_class_pred += [cnn_1st_ind[i]]\n",
    "        else:\n",
    "            # otherwise, there is not a clear win, so we use knn results to help make final decision\n",
    "            if pred_yp_knn_s1[i] == cnn_1st_ind[i]:\n",
    "                combined_class_pred += [cnn_1st_ind[i]]\n",
    "            elif pred_yp_knn_s1[i] == cnn_2st_ind[i]:\n",
    "                combined_class_pred += [cnn_2st_ind[i]]\n",
    "            elif pred_yp_knn_s1[i] == cnn_3st_ind[i]:\n",
    "                combined_class_pred += [cnn_3st_ind[i]]\n",
    "            else:\n",
    "                combined_class_pred += [cnn_1st_ind[i]]\n",
    "\n",
    "    combined_acc_score = accuracy_score(y_test, combined_class_pred)\n",
    "    print(combined_acc_score)\n",
    "    \n",
    "    # output accuracy and prediction labels\n",
    "    return combined_acc_score, combined_class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_test = np.genfromtxt('X_test.csv', delimiter=',')\n",
    "y_test = np.genfromtxt('y_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "acc_score, pred_label = test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
